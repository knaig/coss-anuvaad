{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0063fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "nb_dir = '/home/sriharimn/anuvaad-toolkit/bm1/anuvaad/anuvaad-etl/anuvaad-extractor/document-converter/src'\n",
    "sys.path.append(nb_dir)\n",
    "sys.path.append(os.path.split(nb_dir)[0])\n",
    "\n",
    "import requests\n",
    "import config\n",
    "# import os\n",
    "import re\n",
    "import base64\n",
    "import json\n",
    "import pandas as pd\n",
    "from utilities.utils import DocumentUtilities\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.shared import Twips, Cm,Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_BREAK\n",
    "from docx.enum.section import WD_SECTION, WD_ORIENT\n",
    "from docx.shared import Length\n",
    "from utilities import MODULE_CONTEXT\n",
    "from anuvaad_auditor.loghandler import log_info, log_exception\n",
    "from zipfile import ZipFile\n",
    "import uuid\n",
    "import xlsxwriter\n",
    "from jsonpath_rw import jsonpath, parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf00c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# internal url\n",
    "CONTENT_HANDLER_ENDPOINT    = os.environ.get('CONTENT_HANDLER_SERVER_URL', 'https://auth.anuvaad.org/')\n",
    "FILE_CONVERTER_ENDPOINT     = os.environ.get('FILE_CONVERTER_SERVER_URL', 'http://gateway_anuvaad-file-converter:5001/')\n",
    "\n",
    "\n",
    "OCR_CONTENT_HANDLER_HOST    = os.environ.get('OCR_CONTENT_HANDLER_SERVER_URL', 'https://auth.anuvaad.org/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5fa0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('/home/sriharimn/anuvaad-toolkit/bm1/anuvaad/anuvaad-etl/anuvaad-extractor/document-converter/src/upload/0-16684243414101825.json')\n",
    "\n",
    "\n",
    "# dict_json_data = json.load(f)\n",
    "# print(dict_json_data['result'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518820be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "auth_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyIjoic3JpaDg1NCIsImV4cCI6MTY2OTQ1NDQ2Mn0.RBffCdkq2zWnaXsm8G_VPXrf8484mixPxE2g3UxF8IQ\"\n",
    "# f = open('/home/sriharimn/anuvaad-toolkit/bm1/anuvaad/anuvaad-etl/anuvaad-extractor/document-converter/src/upload/0-16684243414101825.json')\n",
    "\n",
    "# data = json.load(f)\n",
    "\n",
    "\n",
    "class DocumentConversion(object):\n",
    "\n",
    "    def __init__(self, DOWNLOAD_FOLDER):\n",
    "        self.DOWNLOAD_FOLDER = DOWNLOAD_FOLDER\n",
    "\n",
    "    # getting document json data from fetch-content end point of content-handler.\n",
    "    def get_data_from_content_handler(self, record_id, user_id, start_page=0, end_page=0):\n",
    "        doc_utils = DocumentUtilities()\n",
    "        try:\n",
    "            headers = {\"x-user-id\" : user_id, \"Content-Type\": \"application/json\",\"auth-token\":auth_token}\n",
    "            request_url = doc_utils.url_generation(config.CONTENT_HANDLER_ENDPOINT, record_id, start_page, end_page)\n",
    "            log_info(\"Intiating request to fetch data from %s\"%request_url, MODULE_CONTEXT)\n",
    "            response = requests.get(request_url, headers = headers)\n",
    "            response_data = response.content\n",
    "            log_info(\"Received data from fetch-content end point of content handler\", MODULE_CONTEXT)\n",
    "            dict_str = response_data.decode(\"UTF-8\")\n",
    "            dict_json_data = json.loads(dict_str)\n",
    "\n",
    "\n",
    "#             dict_json_data = json.load(f)\n",
    "#             print(dict_json_data)\n",
    "            return dict_json_data\n",
    "        except Exception as e:\n",
    "            log_exception(\"Can not fetch content in content handler: \", MODULE_CONTEXT, e)\n",
    "\n",
    "    # converting document json data into pandas dataframes.\n",
    "    def convert_page_data_into_dataframes(self, pages):\n",
    "        # try:\n",
    "            dfs              = []\n",
    "            page_width       = None\n",
    "            page_height      = None\n",
    "            page_layout = {}\n",
    "            for page in pages:\n",
    "                text_tops        = []\n",
    "                text_lefts       = []\n",
    "                text_widths      = []\n",
    "                text_heights     = []\n",
    "                font_sizes       = []\n",
    "                font_families    = []\n",
    "                font_colors      = []\n",
    "                text_values      = []\n",
    "                b64_images       = []\n",
    "                if 'images' not in list(page.keys()) or 'text_blocks' not in list(page.keys()):\n",
    "                    log_info('looks like one of the key is missing {}'.format(page.keys()), MODULE_CONTEXT)\n",
    "                    pass\n",
    "                \n",
    "                images       = page['images'] if 'images' in list(page.keys()) else None\n",
    "                texts        = page['text_blocks']\n",
    "                page_num     = page['page_no']\n",
    "                page_width   = page['page_width']\n",
    "                page_height  = page['page_height']\n",
    "                page_layout.update({'page_width' : page_width, 'page_height' : page_height})\n",
    "                for text in texts:\n",
    "                    text_tops.append(text['text_top'])\n",
    "                    text_lefts.append(text['text_left'])\n",
    "                    text_widths.append(text['text_width'])\n",
    "                    text_heights.append(text['text_height'])\n",
    "                    font_sizes.append(text['font_size'])\n",
    "                    font_families.append(text['font_family'])\n",
    "                    font_colors.append(text['font_color'])\n",
    "                    b64_images.append(None)\n",
    "                    \n",
    "                    text_value = []\n",
    "                    for processed_text in text['tokenized_sentences']:\n",
    "                        # print(processed_text)\n",
    "                        if 'tgt' in processed_text:\n",
    "                            if processed_text['tgt'] != None:\n",
    "                                # print(processed_text['tgt'],\"tgtttttttttttttttttttt\")\n",
    "                                text_value.append(processed_text['tgt'])\n",
    "                            else:\n",
    "                                text_value.append(\"This line is Not Translated\")\n",
    "                        else:\n",
    "                            # print(processed_text['src'],\"srcccccccccccccc\")\n",
    "                            text_value.append(processed_text['src'])\n",
    "                            # print(processed_text['src'],\"###########################\")\n",
    "                    # print(text_value,\"**************\")\n",
    "                    if text_value  :\n",
    "                        text_values.append(' '.join(text_value))\n",
    "                if images:                       \n",
    "                    for image in images:\n",
    "                        text_tops.append(image['text_top'])\n",
    "                        text_lefts.append(image['text_left'])\n",
    "                        text_widths.append(image['text_width'])\n",
    "                        text_heights.append(image['text_height'])\n",
    "                        b64_images.append(image['base64'])\n",
    "                        text_values.append(None)\n",
    "                        font_sizes.append(None)\n",
    "                        font_families.append(None)\n",
    "                        font_colors.append(None)\n",
    "                \n",
    "                df = pd.DataFrame(list(zip(text_tops, text_lefts, text_widths, text_heights,\n",
    "                                                        text_values, font_sizes, font_families, font_colors, b64_images)), \n",
    "                                        columns =['text_top', 'text_left', 'text_width', 'text_height',\n",
    "                                                    'text', 'font_size', 'font_family', 'font_color', 'base64'])\n",
    "                df.sort_values('text_top', axis = 0, ascending = True, inplace=True) \n",
    "                df = df.reset_index()\n",
    "                df = df.where(pd.notnull(df), None)\n",
    "                dfs.append(df)\n",
    "            log_info(\"dataframes formed\", MODULE_CONTEXT)\n",
    "            return dfs, page_layout\n",
    "        # except Exception as e:\n",
    "        #     log_exception(\"dataframe formation error\", MODULE_CONTEXT, e)\n",
    "\n",
    "    # using dataframe of document json data to create docx file of target sentences. \n",
    "    def document_creation(self, dataframes, page_layout, record_id):\n",
    "        # try:\n",
    "            doc_utils = DocumentUtilities()\n",
    "            document              = Document()\n",
    "            section               = document.sections[-1]\n",
    "#             print(section)\n",
    "            section.orientation   = WD_ORIENT.PORTRAIT\n",
    "            \n",
    "            section.page_width    = Cm(doc_utils.get_cms(page_layout['page_width']))\n",
    "            section.page_height   = Cm(doc_utils.get_cms(page_layout['page_height']))\n",
    "            print(section.page_height)\n",
    "            print(section.page_width)\n",
    "            print(page_layout['page_width'], \"width\")\n",
    "            print(page_layout['page_height'],\"height\")\n",
    "            \n",
    "#             section.left_margin   = Cm(1.27)\n",
    "#             section.right_margin  = Cm(1.27)\n",
    "#             section.top_margin    = Cm(1.27)\n",
    "#             section.bottom_margin = Cm(1.27)\n",
    "            document._body.clear_content()\n",
    "            \n",
    "            for index, df in enumerate(dataframes):\n",
    "                for index, row in df.iterrows():\n",
    "#                     if row['text'] == None and row['base64'] != None:\n",
    "#                         image_path = doc_utils.get_path_from_base64(self.DOWNLOAD_FOLDER, row['base64'])           \n",
    "#                         document.add_picture(image_path, width=Cm(doc_utils.get_cms(row['text_width'])), \n",
    "#                                         height=Cm(doc_utils.get_cms(row['text_height'])))\n",
    "#                         os.remove(image_path)\n",
    "                    if row['text'] != None and row['base64'] == None:\n",
    "                        paragraph                      = document.add_paragraph()\n",
    "                        paragraph_format               = paragraph.paragraph_format\n",
    "                        paragraph_format.alignment = None\n",
    "#                         paragraph_format.left_indent   = Cm(doc_utils.get_cms(row['text_width']-row['text_left']))\n",
    "#                         paragraph_format.left_indent   = Cm(doc_utils.get_cms(row['text_left']))\n",
    "                        if index != df.index[-1] and df.iloc[index + 1]['text_top'] != row['text_top'] and row['font_size'] !=None:\n",
    "                            # print(df.iloc[index + 1]['text_top'] - row['text_top'] - row['font_size'],\"indexxxxx\")\n",
    "                            print(page_layout['page_width']-row['text_top'])\n",
    "                            pixel = page_layout['page_width']-(df.iloc[index + 1]['text_top'] - row['text_top'] - row['font_size'])\n",
    "#                             print(pixel,\"pixels\")\n",
    "                            if pixel>0:\n",
    "                                paragraph_format.space_after = Twips(doc_utils.pixel_to_twips(pixel))\n",
    "                            else:\n",
    "                                paragraph_format.space_after = Twips(0)\n",
    "                        else:\n",
    "                            paragraph_format.space_after = Twips(0)\n",
    "#                         print(paragraph_format)\n",
    "                        run                            = paragraph.add_run()\n",
    "                        if row['font_family'] != None and \"Bold\" in row['font_family']:\n",
    "                            run.bold                   = True\n",
    "                        font                           = run.font\n",
    "                        font.name                      = 'Arial'\n",
    "                        if row['font_size'] != None:\n",
    "                            font.size                      = Twips(doc_utils.pixel_to_twips(row['font_size'])) \n",
    "                        run.add_text(row['text'])\n",
    "                run.add_break(WD_BREAK.PAGE)\n",
    "            out_filename = os.path.splitext(os.path.basename(record_id.split('|')[0]))[0] + str(uuid.uuid4()) + '_translated_docx.docx'\n",
    "            output_filepath = os.path.join(self.DOWNLOAD_FOLDER , out_filename)\n",
    "            document.save(output_filepath)\n",
    "#             out_filename_zip = zipfile_creation(output_filepath)\n",
    "            log_info(\"docx file formation done!! filename: %s\"%output_filepath, MODULE_CONTEXT)\n",
    "            return output_filepath\n",
    "        # except Exception as e:\n",
    "        #     log_exception(\"dataframe to doc formation failed\", MODULE_CONTEXT, e)\n",
    "\n",
    "    # get all tokenised object from document json data into one list.\n",
    "    def get_tokenized_sentences(self, json_data):\n",
    "        try:\n",
    "            jsonpath_expr = parse('$..tokenized_sentences[*]')\n",
    "            matches       = jsonpath_expr.find(json_data)\n",
    "            tokenized_sentences = []\n",
    "            for match in matches:\n",
    "                tokenized_sentences.append(match.value)\n",
    "            return tokenized_sentences\n",
    "        except Exception as e:\n",
    "            log_exception(\"Getting only tokenised sentence object failed\", MODULE_CONTEXT, e)\n",
    "\n",
    "    # create xlsx file of source sentence in one column and taget sentences in adjacent column\n",
    "    def generate_xlsx_file(self, record_id, json_data):\n",
    "        try:\n",
    "            out_xlsx_filename = os.path.splitext(os.path.basename(record_id.split('|')[0]))[0] + str(uuid.uuid4()) + '_src_tgt_xlsx.xlsx'\n",
    "            output_filepath_xlsx = os.path.join(self.DOWNLOAD_FOLDER , out_xlsx_filename)\n",
    "            workbook = xlsxwriter.Workbook(output_filepath_xlsx)\n",
    "            worksheet = workbook.add_worksheet()\n",
    "            worksheet.write('A1', 'Source Sentence') \n",
    "            worksheet.write('B1', 'Target Sentence')\n",
    "            row, column = 1, 0\n",
    "            tokenised_sentences = self.get_tokenized_sentences(json_data)\n",
    "            if tokenised_sentences != [] or tokenised_sentences != None:\n",
    "                for tokenised_sentence in tokenised_sentences:\n",
    "                    if tokenised_sentence != [] or tokenised_sentence != None:\n",
    "                        worksheet.write(row, column, tokenised_sentence['src']) \n",
    "                        worksheet.write(row, column + 1, tokenised_sentence['tgt']) \n",
    "                        row += 1\n",
    "            workbook.close()\n",
    "            out_xlsx_filename_zip = zipfile_creation(output_filepath_xlsx)\n",
    "            log_info(\"xlsx file write completed!! filename: %s\"%out_xlsx_filename_zip, MODULE_CONTEXT)\n",
    "            return out_xlsx_filename_zip\n",
    "        except Exception as e:\n",
    "            log_exception(\"xlsx file formation failed\", MODULE_CONTEXT, e)\n",
    "\n",
    "    # breaking large sentences into page width fit sentences\n",
    "    def break_large_sentence(self, sentence, max_char_in_line):\n",
    "        try:\n",
    "            sub_str_list = [sentence[i:i+max_char_in_line] for i in range(0, len(sentence), max_char_in_line)]\n",
    "            for idx, sub_str in enumerate(sub_str_list):\n",
    "                if idx+1 < len(sub_str_list):\n",
    "                    if not sub_str.endswith(' ') or not sub_str_list[idx+1].startswith(' '): \n",
    "                        sub_str_split = sub_str.split(' ')\n",
    "                        last_word_sub_str = sub_str_split[-1]\n",
    "                        next_sub_str_split = sub_str_list[idx+1].split(' ')\n",
    "                        first_word_sub_str = next_sub_str_split[0]\n",
    "                        if len(last_word_sub_str) < len(first_word_sub_str):\n",
    "                            next_sub_str_split[0] = ' ' + last_word_sub_str + first_word_sub_str\n",
    "                            del sub_str_split[-1]\n",
    "                        else:\n",
    "                            sub_str_split[-1] = last_word_sub_str + first_word_sub_str + ' '\n",
    "                            del next_sub_str_split[0]\n",
    "                        sub_str_list[idx] = ' '.join(sub_str_split)\n",
    "                        sub_str_list[idx+1] = ' '.join(next_sub_str_split)\n",
    "            return sub_str_list\n",
    "        except Exception as e:\n",
    "            log_exception(\"sentence breaking failed for txt file writing\", MODULE_CONTEXT, e)\n",
    "\n",
    "    # create txt file for translated sentences \n",
    "    def create_translated_txt_file(self, record_id, dataframes, page_layout):\n",
    "        try:\n",
    "            out_translated_txt_filename = os.path.splitext(os.path.basename(record_id.split('|')[0]))[0] + str(uuid.uuid4()) + '_translated_txt.txt'\n",
    "            output_filepath_txt = os.path.join(self.DOWNLOAD_FOLDER , out_translated_txt_filename)\n",
    "            out_txt_file_write = open(output_filepath_txt, 'w')\n",
    "            max_chars_in_line = int(page_layout['page_width']/13) if page_layout['page_width'] else 500\n",
    "            for idx, df in enumerate(dataframes):\n",
    "                for idx, row in df.iterrows():\n",
    "                    if df.iloc[idx]['text'] != None and idx+1 < df.shape[0]:\n",
    "                        extra_spaces = int((df.iloc[idx]['text_left'] - 50)/13) if df.iloc[idx]['text_left'] else 50\n",
    "                        write_str = re.sub(r'^', ' '*extra_spaces, df.iloc[idx]['text'])\n",
    "                        if df.iloc[idx]['text_top'] != df.iloc[idx+1]['text_top']:\n",
    "                            if len(write_str) < max_chars_in_line:\n",
    "                                out_txt_file_write.write(\"%s\\n\"%write_str)\n",
    "                            else:\n",
    "                                sub_string_list = self.break_large_sentence(write_str, max_chars_in_line)\n",
    "                                for item in sub_string_list:\n",
    "                                    out_txt_file_write.write(\"%s\\n\"%item)\n",
    "                        else:\n",
    "                            same_line_index = 0\n",
    "                            same_line_status = bool(df.iloc[idx]['text_top'] == df.iloc[idx+same_line_index+1]['text_top'])\n",
    "                            while same_line_status:\n",
    "                                if idx+same_line_index+1 < df.shape[0]:\n",
    "                                    \n",
    "                                    try:\n",
    "                                        onwards_line_space =    int((df.iloc[idx+same_line_index+1]['text_left'] - df.iloc[idx+same_line_index]['text_left'] \\\n",
    "                                                            - df.iloc[idx+same_line_index]['text_width'])/13)\n",
    "                                    except:\n",
    "                                        onwards_line_space = 50\n",
    "\n",
    "                                    if df.iloc[idx+same_line_index+1]['text'] != None:\n",
    "                                        write_str += ' '*onwards_line_space + df.iloc[idx+same_line_index+1]['text']\n",
    "                                        df = df.replace({df.iloc[idx+same_line_index+1]['text'] : None})\n",
    "                                    else:\n",
    "                                        write_str += ' '*onwards_line_space + ''\n",
    "                                    same_line_index += 1\n",
    "                                    if idx+same_line_index+1 < df.shape[0]:\n",
    "                                        same_line_status = bool(df.iloc[idx+same_line_index]['text_top'] == df.iloc[idx+same_line_index+1]['text_top'])\n",
    "                                    else:\n",
    "                                        same_line_status = False\n",
    "                                else:\n",
    "                                    same_line_status = False\n",
    "                            if len(write_str) < max_chars_in_line:\n",
    "                                out_txt_file_write.write(\"%s\\n\"%write_str)\n",
    "                            else:\n",
    "                                sub_string_list = self.break_large_sentence(write_str, max_chars_in_line)\n",
    "                                for item in sub_string_list:\n",
    "                                    out_txt_file_write.write(\"%s\\n\"%item)\n",
    "                    elif df.iloc[idx]['text'] != None and idx+1 == df.shape[0]:\n",
    "                        extra_spaces = int((df.iloc[idx]['text_left'] - 50)/13) if df.iloc[idx]['text_left'] else 50\n",
    "                        write_str = re.sub(r'^', ' '*extra_spaces, df.iloc[idx]['text'])\n",
    "                        if len(write_str) < max_chars_in_line:\n",
    "                            out_txt_file_write.write(\"%s\\n\"%write_str)\n",
    "                        else:\n",
    "                            sub_string_list = self.break_large_sentence(write_str, max_chars_in_line)\n",
    "                            for item in sub_string_list:\n",
    "                                out_txt_file_write.write(\"%s\\n\"%item)\n",
    "            out_txt_file_write.close()\n",
    "            out_txt_zip = zipfile_creation(output_filepath_txt)\n",
    "            log_info(\"txt file write completed!! filename: %s\"%out_txt_zip, MODULE_CONTEXT)\n",
    "            return out_txt_zip\n",
    "        except Exception as e:\n",
    "            log_exception(\"txt file formation failed\", MODULE_CONTEXT, e)\n",
    "            \n",
    "def zipfile_creation(filepath):\n",
    "    arcname = filepath.replace(f\"{config.DATA_OUTPUT_DIR}/\",\"\")\n",
    "    zip_file = filepath.split('.')[0] + '.zip'\n",
    "    with ZipFile(zip_file, 'w') as myzip:\n",
    "        myzip.write(filepath,arcname)\n",
    "    os.remove(filepath)\n",
    "    return zip_file.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9308028b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f40ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_saving(record_id, user_id, download_folder, file_type):\n",
    "#     try:\n",
    "        log_info(\"document type %s formation started\"%file_type, MODULE_CONTEXT)\n",
    "        doc_conversion = DocumentConversion(download_folder)\n",
    "        json_data = doc_conversion.get_data_from_content_handler(record_id, user_id)\n",
    "#         print(json_data)\n",
    "        dataframes, page_layout = doc_conversion.convert_page_data_into_dataframes(json_data['data'])\n",
    "        if file_type == 'docx':\n",
    "            output_filename = doc_conversion.document_creation(dataframes, page_layout, record_id)\n",
    "            return output_filename\n",
    "        elif file_type == 'xlsx':\n",
    "            xlsx_filename = doc_conversion.generate_xlsx_file(record_id, json_data)\n",
    "            return xlsx_filename\n",
    "        elif file_type == 'txt':\n",
    "            txt_filename = doc_conversion.create_translated_txt_file(record_id, dataframes, page_layout)\n",
    "            return txt_filename\n",
    "#     except:\n",
    "#         log_exception(\"Document type %s saving failed\"%file_type, MODULE_CONTEXT, None)\n",
    "#         raise ServiceError(400, \"Document type %s saving failed\"%file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28304b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_id = \"A_FBTTR-xljdu-1668422013063|0-16684243414101825.json\"\n",
    "user_id = \"fe2781567e12417e857a56a04fd31a011654250971559\"\n",
    "file_type = \"txt\"\n",
    "download_folder = \"/home/sriharimn/anuvaad-toolkit/bm1/anuvaad/anuvaad-etl/anuvaad-extractor/document-converter/src/upload\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f492d13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-26 07:12:36,327] {loghandler.py:21} MainThread INFO in loghandler: document type txt formation started\n",
      "[2022-11-26 07:12:36,332] {loghandler.py:21} MainThread INFO in loghandler: Intiating request to fetch data from https://auth.anuvaad.org//anuvaad/content-handler/v0/fetch-content?record_id=A_FBTTR-xljdu-1668422013063|0-16684243414101825.json&start_page=0&end_page=0\n",
      "[2022-11-26 07:14:24,261] {loghandler.py:21} MainThread INFO in loghandler: Received data from fetch-content end point of content handler\n",
      "[2022-11-26 07:14:25,805] {loghandler.py:21} MainThread INFO in loghandler: dataframes formed\n",
      "[2022-11-26 07:14:34,850] {loghandler.py:21} MainThread INFO in loghandler: txt file write completed!! filename: A_FBTTR-xljdu-1668422013063872fdf07-d103-4ad8-b983-22364acfd9b2_translated_txt.zip\n"
     ]
    }
   ],
   "source": [
    "document_converter = document_saving(record_id, user_id, download_folder, file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "381d9e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "print(isNaN(\"hello\"))\n",
    "print(isNaN(1))\n",
    "print(isNaN(np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15eed63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
