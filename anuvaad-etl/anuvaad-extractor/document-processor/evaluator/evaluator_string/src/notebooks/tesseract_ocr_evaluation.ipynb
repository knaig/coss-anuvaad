{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naresh/ds-env2/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import uuid\n",
    "import json\n",
    "import requests\n",
    "import copy\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import logging\n",
    "from collections import Counter\n",
    "from pytesseract import Output\n",
    "from pytesseract import pytesseract\n",
    "from difflib import SequenceMatcher\n",
    "import time\n",
    "from polyfuzz import PolyFuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/home/srihari/Desktop/data/data'\n",
    "ocr_level = \"LINE\"\n",
    "text_processing = True\n",
    "REJECT_FILTER = 2\n",
    "crop_factor= 7\n",
    "crop_factor_y= 4\n",
    "crop_save = True\n",
    "digitization = True\n",
    "vis_thresh=0.90\n",
    "LANG_MAPPING       =  {\n",
    "    \"en\" : [\"Latin\",\"eng\"],\n",
    "    \"kn\" : ['Kannada',\"kan\"],\n",
    "    \"gu\": [\"guj\"],\n",
    "    \"or\": [\"ori\"],\n",
    "    \"hi\" : [\"Devanagari\",\"hin\",\"eng\"],\n",
    "    \"bn\" : [\"Bengali\",\"ben\"],\n",
    "    \"mr\": [\"Devanagari\",\"hin\",\"eng\"],\n",
    "    \"ta\": ['Tamil',\"tam\"],\n",
    "    \"te\" : [\"Telugu\",\"tel\"],\n",
    "    \"ml\" :[\"Malayalam\"],\n",
    "    \"ma\" :[\"Marathi\"]\n",
    "}\n",
    "path = '/home/naresh/Tarento/testing_document_processor/test_pipeline/data/'\n",
    "output_path = '/home/naresh/Tarento/testing_document_processor/test_pipeline/result/'\n",
    "output_path_boxes= '/home/naresh/Tarento/testing_document_processor/test_word_boxes/'\n",
    "base_path= '/home/naresh/Tarento/testing_document_processor/test_word_boxes/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyTmFtZSI6ImRoaXJhai5kYWdhQHRhcmVudG8uY29tIiwicGFzc3dvcmQiOiJiJyQyYiQxMiRmaXZHbDU5N1VuRG14WHNiSDA2TzdPMDlDdHl0MW96YnN1eFNVV0JoUERXbzVHZ2FGWS5pcSciLCJleHAiOjE2MTk1OTU4MTN9.FFtVz_M3Ni6rJJcl7_mSC0dPoVfU5gB3apsUvrNle4c'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/home/srihari/Desktop/data/data'\n",
    "word_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
    "google_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
    "layout_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
    "segmenter_url = \"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/async/initiate\"\n",
    "bs_url =\"https://auth.anuvaad.org/anuvaad-etl/wf-manager/v1/workflow/jobs/search/bulk\"\n",
    "\n",
    "evaluator_url  = \"https://auth.anuvaad.org/anuvaad-etl/document-processor/evaluator/v0/process\"\n",
    "\n",
    "#evaluator_url = 'http://0.0.0.0:5001/anuvaad-etl/document-processor/evaluator/v0/process'\n",
    "\n",
    "download_url =\"https://auth.anuvaad.org/download/\"\n",
    "upload_url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'auth-token' :token }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Draw:\n",
    "    \n",
    "    def __init__(self,input_json,save_dir,regions,prefix='',color= (255,0,0),thickness=5):   \n",
    "        self.json = input_json\n",
    "        self.save_dir = save_dir\n",
    "        self.regions = regions\n",
    "        self.prefix  = prefix\n",
    "        self.color  = color\n",
    "        self.thickness=thickness\n",
    "        if self.prefix == 'seg':\n",
    "            #print('drawing children')\n",
    "            self.draw_region_children()\n",
    "        else:\n",
    "            self.draw_region__sub_children()\n",
    "        \n",
    "    def get_coords(self,page_index):\n",
    "        return self.json['outputs'][0]['pages'][page_index][self.regions]\n",
    "    \n",
    "    def get_page_count(self):\n",
    "        return(self.json['outputs'][0]['page_info'])\n",
    "    \n",
    "    def get_page(self,page_index):\n",
    "        page_path = self.json['outputs'][0]['page_info'][page_index]\n",
    "        page_path = page_path.split('upload')[1]#'/'.join(page_path.split('/')[1:])\n",
    "        #print(page_path)    \n",
    "        return download_file(download_url,headers,page_path,f_type='image')\n",
    "\n",
    "    def draw_region(self):\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "        for page_index in range(len(self.get_page_count())) :\n",
    "            nparr = np.frombuffer(self.get_page(page_index), np.uint8)\n",
    "            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "            for region in self.get_coords(page_index) :\n",
    "                    ground = region['boundingBox']['vertices']\n",
    "                    pts = []\n",
    "                    for pt in ground:\n",
    "                        pts.append([int(pt['x']) ,int(pt['y'])])\n",
    "                    cv2.polylines(image, [np.array(pts)],True, self.color, self.thickness)\n",
    "                    if 'class' not in region.keys():\n",
    "                        region['class'] = 'TEXT'\n",
    "                    cv2.putText(image, str(region['class']), (pts[0][0],pts[0][1]), font,  \n",
    "                   2, (0,125,255), 3, cv2.LINE_AA)\n",
    "                    \n",
    "            image_path = os.path.join(self.save_dir ,  '{}_{}_{}.png'.format(self.regions,self.prefix,page_index))            \n",
    "            cv2.imwrite(image_path , image)\n",
    "          \n",
    "    def draw_region_children(self):\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "        fontScale = 2\n",
    "        thickness =3\n",
    "\n",
    "\n",
    "        for page_index in range(len(self.get_page_count())) :\n",
    "            nparr = np.frombuffer(self.get_page(page_index), np.uint8)\n",
    "            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "            for region_index,region in enumerate(self.get_coords(page_index)) :\n",
    "                try:\n",
    "                    ground = region['boundingBox']['vertices']\n",
    "                    pts = []\n",
    "                    for pt in ground:\n",
    "                        pts.append([int(pt['x']) ,int(pt['y'])])\n",
    "                    #print(pts)\n",
    "                    region_color = (0 ,0,125+ 130*(region_index/ len(self.get_coords(page_index))))\n",
    "                    cv2.polylines(image, [np.array(pts)],True, region_color, self.thickness)\n",
    "                    cv2.putText(image, str(region_index), (pts[0][0],pts[0][1]), font,  \n",
    "                   fontScale, region_color, thickness, cv2.LINE_AA)\n",
    "                    for line_index, line in enumerate(region['children']):\n",
    "                        ground = line['boundingBox']['vertices']\n",
    "                        pts = []\n",
    "                        for pt in ground:\n",
    "                            pts.append([int(pt['x']) ,int(pt['y'])])\n",
    "\n",
    "                        line_color = (125 + 130*(region_index/ len(self.get_coords(page_index))) ,0,0)\n",
    "                        cv2.polylines(image, [np.array(pts)],True, line_color, self.thickness -2)\n",
    "                        cv2.putText(image, str(line_index), (pts[0][0],pts[0][1]), font,  \n",
    "                   fontScale, line_color, thickness, cv2.LINE_AA)\n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                    print(region)\n",
    "                    \n",
    "            image_path = os.path.join(self.save_dir ,  '{}_{}.png'.format(self.prefix,page_index))\n",
    "            cv2.imwrite(image_path , image)\n",
    "    def draw_region__sub_children(self):        \n",
    "        for page_index in range(len(self.get_page_count())) :\n",
    "            nparr = np.frombuffer(self.get_page(page_index), np.uint8)\n",
    "            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "            print(image)\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "            fontScale = 2\n",
    "\n",
    "            # Blue color in BGR \n",
    "            color = (0 ,255,0) \n",
    "\n",
    "            # Line thickness of 2 px \n",
    "            thickness = 3\n",
    "\n",
    "            # Using cv2.putText() method \n",
    "            \n",
    "            for region_index,region in enumerate(self.get_coords(page_index)) :\n",
    "                try:\n",
    "                    ground = region['boundingBox']['vertices']\n",
    "                    pts = []\n",
    "                    for pt in ground:\n",
    "                        pts.append([int(pt['x']) ,int(pt['y'])])\n",
    "                    #print(pts)\n",
    "                    region_color = (0,0,255)\n",
    "                    cv2.polylines(image, [np.array(pts)],True, region_color, self.thickness)\n",
    "                    for line_index, line in enumerate(region['regions']):\n",
    "                        ground = line['boundingBox']['vertices']\n",
    "                        pts = []\n",
    "                        for pt in ground:\n",
    "                            pts.append([int(pt['x'])-1 ,int(pt['y']) -1 ])\n",
    "\n",
    "                        line_color = (255,0,0)\n",
    "                        cv2.polylines(image, [np.array(pts)],True, line_color, self.thickness -2)\n",
    "                        \n",
    "                        cv2.putText(image, str(line_index), (pts[0][0],pts[0][1]), font,  \n",
    "                   fontScale, (255,0,0), thickness, cv2.LINE_AA)\n",
    "                        for word_index, word in enumerate(line['regions']):\n",
    "                            ground = word['boundingBox']['vertices']\n",
    "                            pts = []\n",
    "                            for pt in ground:\n",
    "                                pts.append([int(pt['x']) -3,int(pt['y'])-3])\n",
    "\n",
    "                            word_color = (0,255,0)\n",
    "                            cv2.polylines(image, [np.array(pts)],True, word_color, self.thickness -2)\n",
    "\n",
    "                            cv2.putText(image, str(word_index), (pts[0][0],pts[0][1]), font,  \n",
    "                       fontScale-1,(0,255,0), thickness, cv2.LINE_AA)\n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                    print(region)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            #print(self.prefix)\n",
    "            image_path = os.path.join(self.save_dir ,  '{}_{}_{}.png'.format(self.prefix,self.regions,page_index))\n",
    "            cv2.imwrite(image_path , image)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# google vision pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_ocr_v15(url,headers,pdf_name):\n",
    "    \n",
    "    file = {\n",
    "       \"files\": [\n",
    "        {\n",
    "            \"locale\": \"en\",\n",
    "            \"path\": pdf_name,\n",
    "            \"type\": \"pdf\",\n",
    "            \"config\":{\n",
    "        \"OCR\": {\n",
    "          \"option\": \"HIGH_ACCURACY\",\n",
    "          \"language\": \"en\",\n",
    "          \"top_correction\":\"True\",\n",
    "          \"craft_word\": \"True\",\n",
    "          \"craft_line\": \"True\",\n",
    "        }\n",
    "        }}\n",
    "    ],\n",
    "    \"workflowCode\": \"WF_A_FCWDLDBSOD15GV\"\n",
    "    }\n",
    "    res = requests.post(url,json=file,headers=headers)\n",
    "    return res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(pdf_file,headers,url):\n",
    "    #url = 'https://auth.anuvaad.org/anuvaad-api/file-uploader/v0/upload-file'\n",
    "    files = [\n",
    "        ('file',(open(pdf_file,'rb')))] \n",
    "\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "    \n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(download_url,headers,outputfile,f_type='json'):\n",
    "    download_url =download_url+str(outputfile)\n",
    "    res = requests.get(download_url,headers=headers)\n",
    "    if f_type == 'json':\n",
    "        return res.json()\n",
    "    else :\n",
    "        return res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(path,res):\n",
    "    with open(path, \"w\", encoding='utf8') as write_file:\n",
    "        json.dump(res, write_file,ensure_ascii=False )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_search(job_id,bs_url,headers):\n",
    "    bs_request = {\n",
    "    \"jobIDs\": [job_id],\n",
    "    \"taskDetails\":\"true\"\n",
    "    }\n",
    "    print(job_id)\n",
    "    res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
    "    print(res.json())\n",
    "    \n",
    "   \n",
    "    while(1):\n",
    "        \n",
    "        in_progress = res.json()['jobs'][0]['status']\n",
    "       \n",
    "        if in_progress == 'COMPLETED':\n",
    "            outputfile = res.json()['jobs'][0]['output'][0]['outputFile']\n",
    "            #print(outputfile)\n",
    "            print(in_progress)\n",
    "            return outputfile\n",
    "            break\n",
    "        sleep(0.5)\n",
    "        print(in_progress)\n",
    "        res = requests.post(bs_url,json=bs_request,headers=headers, timeout = 10000)\n",
    "      \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_module(module,url,input_file,module_code,pdf_dir,overwirte=True , draw=True):\n",
    "    \n",
    "        \n",
    "        \n",
    "        output_path = os.path.join(pdf_dir,'{}.json'.format(module_code))\n",
    "        if os.path.exists(output_path) and not overwirte:\n",
    "            print(' loading *****************{}'.format(module_code ))\n",
    "            with open(output_path,'r') as wd_file :\n",
    "                response = json.load(wd_file)\n",
    "                \n",
    "            wf_res = pdf_dir + '/{}_wf.json'.format(module_code)\n",
    "            with open(wf_res,'r') as wd_file :\n",
    "                json_file = json.load(wd_file) \n",
    "            #json_file = upload_file(output_path,headers,upload_url)['data']\n",
    "        else :\n",
    "            if module_code in ['wd','gv']:\n",
    "                res = upload_file(input_file,headers,upload_url)\n",
    "                print('upload response **********', res)\n",
    "                pdf_name = res['data']\n",
    "                response = module(url,headers,pdf_name)\n",
    "            \n",
    "            else : \n",
    "                response = module(url,headers,input_file)\n",
    "                \n",
    "                if 'eval' in module_code :\n",
    "                    json_file = response['outputFile']\n",
    "                    response = download_file(download_url,headers,json_file)\n",
    "                    save_json(output_path,response)\n",
    "                    return json_file,response\n",
    "                \n",
    "            \n",
    "            print(' response *****************{} {}'.format(module_code ,response ))\n",
    "            job_id = response['jobID']\n",
    "            json_file = bulk_search(job_id,bs_url,headers)\n",
    "            save_json(pdf_dir + '/{}_wf.json'.format(module_code),json_file)   \n",
    "            print('bulk search  response **************',json_file )\n",
    "            response = download_file(download_url,headers,json_file)\n",
    "            save_json(output_path,response)\n",
    "            if draw :\n",
    "                if module_code in ['wd','gv']:\n",
    "                    Draw(response,pdf_dir,regions='lines',prefix=module_code)\n",
    "                else :\n",
    "                     Draw(response,pdf_dir,regions='regions',prefix=module_code)\n",
    "                    \n",
    "        return json_file,response\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate__and_save_input(pdf_files,output_dir,headers,word_url,layout_url,download_url,upload_url,bs_url):\n",
    "    word_responses   = {}\n",
    "    layout_responses = {}\n",
    "    segmenter_responses = []\n",
    "    for pdf in pdf_files:\n",
    "        #try :\n",
    "        pdf_name = pdf.split('/')[-1].split('.')[0]\n",
    "        print(pdf , ' is being processed')\n",
    "        pdf_output_dir = os.path.join(output_dir,pdf_name)\n",
    "        os.system('mkdir -p \"{}\"'.format(pdf_output_dir))\n",
    "\n",
    "\n",
    "        wd_json,_ = execute_module(google_ocr_v15,word_url,input_file=pdf,\\\n",
    "                       module_code='gv',pdf_dir=pdf_output_dir,overwirte=True , draw=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path,headers,word_url,layout_url,download_url,upload_url,bs_url):\n",
    "        pdf_names = glob.glob(path + '/*.pdf')\n",
    "        \n",
    "        \n",
    "        return evaluate__and_save_input(pdf_names,output_path,headers,word_url,layout_url,download_url,upload_url,bs_url)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if digitization:\n",
    "    main(path,headers,word_url,layout_url,download_url,upload_url,bs_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound_coordinate(corrdinate,max):\n",
    "    if corrdinate < 0 :\n",
    "        corrdinate = 0\n",
    "    if corrdinate > max:\n",
    "        corrdinate = max - 2\n",
    "    return int(corrdinate)\n",
    "def get_image_from_box(image, box, height=140):\n",
    "    #box = data['box']\n",
    "    #scale = np.sqrt((box[1, 1] - box[2, 1])**2 + (box[0, 1] - box[3, 1])**2) / height\n",
    "    #print(\"scale is \",scale)\n",
    "    #w = int(np.sqrt((box[0, 0] - box[1, 0])**2 + (box[2, 0] - box[3, 0])**2) / scale)\n",
    "    w = max(abs(box[0, 0] - box[1, 0]),abs(box[2, 0] - box[3, 0]))\n",
    "    height = max(abs(box[0, 1] - box[3, 1]),abs(box[1, 1] - box[2, 1]))\n",
    "    pts1 = np.float32(box)\n",
    "    #w=2266-376\n",
    "    pts2 = np.float32([[0, 0], [w, 0],[w,height],[0,height]])\n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    result_img = cv2.warpPerspective(image,M,(w, height)) #flags=cv2.INTER_NEAREST\n",
    "    return result_img\n",
    "\n",
    "def get_text(path,coord,language,mode_height,save_base_path,psm_val):\n",
    "    try:\n",
    "\n",
    "        path = path.split('upload')[1]\n",
    "\n",
    "        image = download_file(download_url,headers,path,f_type='image')\n",
    "        nparr = np.frombuffer(image, np.uint8)\n",
    "        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        #image   = cv2.imread(\"/home/naresh/crop.jpeg\",0)\n",
    "        height, width,channel = image.shape\n",
    "\n",
    "    #         left = bound_coordinate(coord[0] , width)\n",
    "    #         top = bound_coordinate(coord[1],height )\n",
    "    #         right = bound_coordinate(coord[2] ,width)\n",
    "    #         bottom = bound_coordinate(coord[3], height)\n",
    "    #         region_width = abs(right-left)\n",
    "    #         region_height = abs(bottom-top)\n",
    "\n",
    "    #         if left==right==top==bottom==0 or region_width==0 or region_height==0:\n",
    "    #             return \"\"\n",
    "\n",
    "        crop_image = get_image_from_box(image, coord, height=abs(coord[0,1]-coord[2,1]))\n",
    "        #crop_image = image[ top:bottom, left:right]\n",
    "        save_path  =  save_base_path+\"/\"+str(uuid.uuid4()) + '.jpg'\n",
    "        if crop_save:\n",
    "            cv2.imwrite(save_path,crop_image)\n",
    "        #if abs(bottom-top) > 3*mode_height:\n",
    "        if abs(coord[1,1]-coord[2,1])>3*mode_height:\n",
    "            text = pytesseract.image_to_string(crop_image,config='--psm 6', lang=LANG_MAPPING[language][1])\n",
    "        else:\n",
    "            text = pytesseract.image_to_string(crop_image,config='--psm '+str(psm_val), lang=LANG_MAPPING[language][1])\n",
    "        return text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def merger_text(line):\n",
    "    text = \"\"\n",
    "    for word_idx, word in enumerate(line['regions']):\n",
    "        if \"text\" in word.keys():\n",
    "            text = text+\" \"+ word[\"text\"]\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def get_coord(bbox):\n",
    "    temp_box = []\n",
    "    temp_box_cv = []\n",
    "    temp_box.append([bbox[\"boundingBox\"]['vertices'][0]['x'],bbox[\"boundingBox\"]['vertices'][0]['y']])\n",
    "    temp_box.append([bbox[\"boundingBox\"]['vertices'][1]['x'],bbox[\"boundingBox\"]['vertices'][1]['y']])\n",
    "    temp_box.append([bbox[\"boundingBox\"]['vertices'][2]['x'],bbox[\"boundingBox\"]['vertices'][2]['y']])\n",
    "    temp_box.append([bbox[\"boundingBox\"]['vertices'][3]['x'],bbox[\"boundingBox\"]['vertices'][3]['y']])\n",
    "    \n",
    "    temp_box_cv.append(bbox[\"boundingBox\"]['vertices'][0]['x'])\n",
    "    temp_box_cv.append(bbox[\"boundingBox\"]['vertices'][0]['y'])\n",
    "    temp_box_cv.append(bbox[\"boundingBox\"]['vertices'][2]['x'])\n",
    "    temp_box_cv.append(bbox[\"boundingBox\"]['vertices'][2]['y'])\n",
    "    temp_box = np.array(temp_box)\n",
    "    return temp_box,temp_box_cv\n",
    "def frequent_height(page_info):\n",
    "    text_height = []\n",
    "    if len(page_info) > 0 :\n",
    "        for idx, level in enumerate(page_info):\n",
    "            coord_crop,coord = get_coord(level)\n",
    "            if len(coord)!=0:\n",
    "                text_height.append(abs(coord[3]-coord[1]))\n",
    "        occurence_count = Counter(text_height)\n",
    "        return occurence_count.most_common(1)[0][0]\n",
    "    else :\n",
    "        return  0\n",
    "def remove_space(a):\n",
    "    return a.replace(\" \", \"\")\n",
    "\n",
    "def seq_matcher(tgt_text,gt_text):\n",
    "    tgt_text = remove_space(tgt_text)\n",
    "    gt_text = remove_space(gt_text)\n",
    "    score = SequenceMatcher(None, gt_text, tgt_text).ratio()\n",
    "\n",
    "    matchs = list(SequenceMatcher(None, gt_text, tgt_text).get_matching_blocks())\n",
    "    match_count=0\n",
    "    match_lis = []\n",
    "    for match in matchs:\n",
    "        #match_lis.append(match.size)\n",
    "        #match_count = max(match_lis)\n",
    "        match_count = match_count + match.size\n",
    "    #gt_text_leng = len(gt_text)\n",
    "    #if gt_text_leng==0:\n",
    "      # gt_text_leng=1\n",
    "    #score = (score*match_count)/gt_text_leng\n",
    "    #     if tgt_text == gt_text:\n",
    "    #         score = 1.0\n",
    "    message = {\"ground\":True,\"input\":True}\n",
    "    if score==0.0:\n",
    "        if len(gt_text)>0 and len(tgt_text)==0:\n",
    "            message['input'] = \"text missing in tesseract\"\n",
    "        if len(gt_text)==0 and len(tgt_text)>0:\n",
    "            message['ground'] = \"text missing in google vision\"\n",
    "    if score==1.0 and len(gt_text)==0 and len(tgt_text)==0:\n",
    "        message['ground'] = \"text missing in google vision\"\n",
    "        message['input'] = \"text missing in tesseract\"\n",
    "    return score,message,match_count\n",
    "def count_mismatch_char(gt ,tgt) :\n",
    "    count=0\n",
    "    gt_count = len(gt)\n",
    "    for i,j in zip(gt,tgt):\n",
    "        if i==j:\n",
    "            count=count+1\n",
    "    mismatch_char = abs(gt_count-count)\n",
    "    return mismatch_char\n",
    "def correct_region(region):\n",
    "    box = region['boundingBox']['vertices']\n",
    "    \n",
    "\n",
    "    region['boundingBox']= {'vertices'  : [{'x':box[0]['x']-crop_factor,'y':box[0]['y']-crop_factor_y},\\\n",
    "                                                                 {'x':box[1]['x']+crop_factor,'y':box[1]['y']-crop_factor_y},\\\n",
    "                                                                 {'x':box[2]['x']+crop_factor,'y':box[2]['y']+crop_factor_y},\\\n",
    "                                                                 {'x':box[3]['x']-crop_factor,'y': box[3]['y']+crop_factor_y}]}\n",
    "    return region\n",
    " \n",
    "\n",
    "\n",
    "def sort_line(line):\n",
    "    line['regions'].sort(key=lambda x: x['boundingBox']['vertices'][0]['x'],reverse=False)\n",
    "    return line\n",
    "\n",
    "\n",
    "def cell_ocr(lang, page_path, line,save_base_path,mode_height):\n",
    "    cell_text =\"\"\n",
    "\n",
    "\n",
    "    for word_idx, word in enumerate(line['regions']):\n",
    "        word = correct_region(word)\n",
    "        coord_crop, coord = get_coord(word)\n",
    "        if len(coord)!=0 and abs(coord_crop[1,1]-coord_crop[2,1]) > REJECT_FILTER :\n",
    "            text = get_text(page_path, coord_crop, lang,mode_height,save_base_path,8) \n",
    "            cell_text = cell_text +\" \" +text\n",
    "    return cell_text\n",
    "def text_extraction(df,lang, page_path, regions,save_base_path):\n",
    "    final_score = 0\n",
    "    total_words = 0\n",
    "    total_lines = 0\n",
    "    total_chars = 0\n",
    "    total_match_chars = 0\n",
    "    for idx, level in enumerate(regions):\n",
    "        mode_height = frequent_height(level['regions'])\n",
    "        if ocr_level==\"WORD\":\n",
    "            for line_idx, line in enumerate(level['regions']):\n",
    "               #word_regions = coord_adjustment(page_path, line['regions'],save_base_path)\n",
    "               for word_idx, word in enumerate(line['regions']):\n",
    "                    word = correct_region(word)\n",
    "                    coord_crop, coord = get_coord(word)\n",
    "                    word_text = word['text']\n",
    "                    if len(word_text)>0 and len(coord)!=0 and abs(coord_crop[1,1]-coord_crop[2,1]) > REJECT_FILTER :\n",
    "                        text = get_text(page_path, coord_crop, lang,mode_height,save_base_path,8)\n",
    "                        if text_processing:\n",
    "                            text_list = text.split()\n",
    "                            text = \" \".join(text_list)\n",
    "                        score,message,match_count = seq_matcher(text,word['text'])\n",
    "                        final_score = final_score+score\n",
    "                        total_words = total_words+1\n",
    "                        total_chars = total_chars+len(remove_space(word['text']))\n",
    "                        total_match_chars= total_match_chars+match_count\n",
    "                        word['char_match'] = match_count\n",
    "                        word['tess_text']     = text\n",
    "                        word['score']         = score\n",
    "                        word['message']       = message\n",
    "                        columns = word.keys()\n",
    "                        df2 = pd.DataFrame([word],columns=columns)\n",
    "                        df = df.append(df2, ignore_index=True)\n",
    "                    elif len(word_text)>0:\n",
    "                        score,message,match_count = seq_matcher(\"\",word['text'])\n",
    "                        word['char_match'] = match_count\n",
    "                        word['tess_text']     = \" \"\n",
    "                        word['score']         = score\n",
    "                        word['message']       = message\n",
    "                        columns = word.keys()\n",
    "                        df2 = pd.DataFrame([word],columns=columns)\n",
    "                        df = df.append(df2, ignore_index=True)\n",
    "        if ocr_level==\"LINE\":\n",
    "            for line_idx, line in enumerate(level['regions']):\n",
    "                line = sort_line(line)\n",
    "                line_text = merger_text(line)\n",
    "                line = correct_region(line)\n",
    "                coord_crop, coord = get_coord(line)\n",
    "\n",
    "\n",
    "                if len(line_text)>0 and len(coord)!=0 and abs(coord_crop[1,1]-coord_crop[2,1]) > REJECT_FILTER :\n",
    "                    if 'class' in line.keys() and (line['class']==\"CELL\" or line['class']==\"CELL_TEXT\"):\n",
    "                        text = cell_ocr(lang, page_path, line,save_base_path,mode_height)\n",
    "\n",
    "                    else:\n",
    "                        text = get_text(page_path, coord_crop, lang,mode_height,save_base_path,7)\n",
    "                    \n",
    "                    if text_processing:\n",
    "                        text_list = text.split()\n",
    "                        text = \" \".join(text_list)\n",
    "                    score,message,match_count = seq_matcher(text,line_text)\n",
    "                    final_score = final_score+score\n",
    "                    total_lines = total_lines+1\n",
    "                    total_chars = total_chars+len(remove_space(line_text))\n",
    "                    total_match_chars= total_match_chars+match_count\n",
    "                    line['char_match'] = match_count\n",
    "                    line['tess_text']     = text\n",
    "                    line['text']     = line_text\n",
    "                    line['score']         = score\n",
    "                    line['message']       = message\n",
    "                    columns = line.keys()\n",
    "                    df2 = pd.DataFrame([line],columns=columns)\n",
    "                    df = df.append(df2, ignore_index=True)\n",
    "                elif len(line_text)>0:\n",
    "                    score,message,match_count = seq_matcher(\"\",line_text)\n",
    "                    line['char_match'] = match_count\n",
    "                    line['tess_text']     = \" \"\n",
    "                    line['text']     = line_text\n",
    "                    line['score']         = score\n",
    "                    line['message']       = message\n",
    "                    columns = line.keys()\n",
    "                    df2 = pd.DataFrame([line],columns=columns)\n",
    "                    df = df.append(df2, ignore_index=True)\n",
    "\n",
    "    #return regions,final_score/total_words,df,total_chars,total_match_chars\n",
    "    return regions,final_score/total_lines,df,total_chars,total_match_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files_path = glob.glob(output_path+\"/average/gv.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tesseract(json_files):\n",
    "    \n",
    "    output = []\n",
    "    dfs =[]\n",
    "    for json_file in json_files:\n",
    "        file_name = json_file.split('/')[-1].split('.json')[0]\n",
    "        pdf_name = json_file.split('/')[-2]\n",
    "        print(\"file name--------------------->>>>>>>>>>>>>>>>>>\",pdf_name)\n",
    "        if not os.path.exists(base_path+pdf_name):\n",
    "            os.mkdir(base_path+pdf_name)\n",
    "        save_base_path = base_path+pdf_name\n",
    "        with open(json_file,'r+') as f:\n",
    "            data = json.load(f)\n",
    "        columns = [\"page_path\",\"page_data\",\"file_eval_info\"]\n",
    "        final_df = pd.DataFrame(columns=columns)\n",
    "        Draw(data,save_base_path,regions='regions')\n",
    "        lang = data['outputs'][0]['config']['OCR']['language']\n",
    "        total_page = len(data['outputs'][0]['pages'])\n",
    "        file_score = 0; total_chars_file = 0\n",
    "        file_data = []; total_match_chars_file = 0\n",
    "        page_paths = []\n",
    "        page_data_counts = []\n",
    "        for idx,page_data in enumerate(data['outputs'][0]['pages']):\n",
    "            t1 = time.time()\n",
    "            print(\"processing started for page no. \",idx)\n",
    "            page_path =  page_data['path']\n",
    "            regions = page_data['regions'][1:]\n",
    "            df = pd.DataFrame()\n",
    "            regions,score,df,total_chars,total_match_chars = text_extraction(df,lang, page_path, regions,save_base_path)\n",
    "            file_score = file_score + score\n",
    "            total_chars_file =total_chars_file +total_chars\n",
    "            total_match_chars_file =  total_match_chars_file+total_match_chars\n",
    "            file_data.append(df.to_csv())\n",
    "            page_paths.append(page_path)\n",
    "            char_details = {\"total_chars\":total_chars,\"total_match_chars\":total_match_chars}\n",
    "            page_data_counts.append(char_details)\n",
    "            data['outputs'][0]['pages'][idx][\"regions\"][1:] = copy.deepcopy(regions)\n",
    "            t2 = t1+time.time()\n",
    "            print(\"processing completed for page in {}\".format(t2))\n",
    "        file_eval_info = {\"total_chars\":total_chars_file,\"total_match_chars\":total_match_chars_file,\"score\":total_match_chars_file/total_chars_file}\n",
    "\n",
    "        print(file_eval_info)\n",
    "        final_df[\"page_path\"] = page_paths\n",
    "        final_df[\"page_data\"] = file_data\n",
    "        final_df[\"file_eval_info\"] = [file_eval_info]*len(page_paths)\n",
    "        \n",
    "        print(\"file level evaluation result------------------->>>>>>>>>>>>>>>>>>>>>>>>>>>\",file_eval_info)\n",
    "        data['outputs'][0]['score'] = file_score/total_page\n",
    "        with open(save_base_path+\"/\"+file_name+\".json\", 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "        final_df.to_csv(save_base_path+\"/\"+file_name+'.csv')\n",
    "    return output,final_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output,dfs = tesseract(json_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_thresh_box(df,path,page_index,save_path):\n",
    "    path = path.split('upload')[1]\n",
    "    \n",
    "    image = download_file(download_url,headers,path,f_type='image')\n",
    "    nparr = np.frombuffer(image, np.uint8)\n",
    "    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "    color= (255,0,0);thickness=5\n",
    "    df =df.reset_index()\n",
    "    for row in df.iterrows():\n",
    "        row2 = row[1].to_dict()\n",
    "        boxes = row2['boundingBox']\n",
    "        boxes2 = ast.literal_eval(boxes)\n",
    "        ground = boxes2['vertices']\n",
    "        \n",
    "        pts = []\n",
    "        for pt in ground:\n",
    "            pts.append([int(pt['x']) ,int(pt['y'])])\n",
    "        cv2.polylines(image, [np.array(pts)],True, color, thickness)\n",
    "        cv2.putText(image, str(row2['text']), (pts[0][0],pts[0][1]), font,  \n",
    "       2, (0,0,255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(row2['tess_text']), (pts[1][0],pts[1][1]), font,  \n",
    "       2, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        image_path = os.path.join(save_path ,  '{}.png'.format(page_index))            \n",
    "        cv2.imwrite(image_path , image)\n",
    "\n",
    "def visualize_results(df_paths,thresh):\n",
    "    for df_path in glob.glob(df_paths+\"*/*.csv\"):\n",
    "        save_path = base_path + df_path.split('/')[-2]+\"/\"\n",
    "        df = pd.read_csv(df_path)\n",
    "        for idx,(page_path,page_data) in enumerate(zip(df['page_path'],df['page_data'])):\n",
    "            df_string = StringIO(page_data)\n",
    "            page_df = pd.read_csv(df_string, sep=\",\")\n",
    "            filtered_df = page_df[page_df['score']<thresh]\n",
    "            draw_thresh_box(filtered_df,page_path,idx,save_path)\n",
    "            \n",
    "visualize_results(base_path,vis_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug at local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_text = \"TERRITORYOFDELHI GOVERNMENTOFNATIONALCAPITAL\" #.replace(\" \", \"\")\n",
    "gt_text = \"GOVERNMENTOFNATIONALCAPITALTERRITORYOFDELHI\" #.replace(\" \", \"\")\n",
    "def remove_space(a):\n",
    "    return a.replace(\" \", \"\")\n",
    "score = list(SequenceMatcher(None, gt_text, tgt_text).get_matching_blocks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"GOVERNMENTOFNATIONALCAPITAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longestSubstring(str1,str2):\n",
    "  \n",
    "     # initialize SequenceMatcher object with \n",
    "     # input string\n",
    "    seqMatch = SequenceMatcher(None,str1,str2)\n",
    "  \n",
    "     # find match of longest sub-string\n",
    "     # output will be like Match(a=0, b=0, size=5)\n",
    "    match = seqMatch.find_longest_match(0, len(str1), 0, len(str2))\n",
    "  \n",
    "     # print longest substring\n",
    "    if (match.size!=0):\n",
    "        print (str1[match.a: match.a + match.size]) \n",
    "    else:\n",
    "        print ('No longest common sub-string found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longestSubstring(tgt_text,gt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d =pd.read_csv(\"/home/naresh/Tarento/testing_document_processor/test_word_boxes/average/gv.csv\")\n",
    "#d =pd.read_csv(\"/home/naresh/gv.csv\")\n",
    "d =pd.read_csv(\"/home/naresh/gv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from leven import levenshtein\n",
    "df_string = StringIO(d['file_eval_info'][0])\n",
    "age_df = pd.read_csv(df_string, sep=\",\")\n",
    "# age_df = age_df[age_df['score']<0.90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>{'total_words': 2373</th>\n",
       "      <th>'total_chars': 22072</th>\n",
       "      <th>'total_match_chars': 21862</th>\n",
       "      <th>'score': 0.9904856832185575</th>\n",
       "      <th>'g_total_match_chars': 0</th>\n",
       "      <th>'g_score': 0.0}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [{'total_words': 2373,  'total_chars': 22072,  'total_match_chars': 21862,  'score': 0.9904856832185575,  'g_total_match_chars': 0,  'g_score': 0.0}]\n",
       "Index: []"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>{'total_words': 2373</th>\n",
       "      <th>'total_chars': 22072</th>\n",
       "      <th>'total_match_chars': 21554</th>\n",
       "      <th>'score': 0.9765313519391083</th>\n",
       "      <th>'g_total_match_chars': 0</th>\n",
       "      <th>'g_score': 0.0}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [{'total_words': 2373,  'total_chars': 22072,  'total_match_chars': 21554,  'score': 0.9765313519391083,  'g_total_match_chars': 0,  'g_score': 0.0}]\n",
       "Index: []"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = age_df.iloc[-5].tess_text #.replace(\" \", \"\")\n",
    "s2 = age_df.iloc[-5].text #.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess:\n",
    "{'total_words': 6705\t'total_chars': 52502\t'total_match_chars': 52161\t'score': 0.9935050093329778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tam_v4:\n",
    "{'total_words': 6705\t'total_chars': 52502\t'total_match_chars': 52113\t'score': 0.9925907584472973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar=\n",
    "less= 1+1+1+1+1+1+1\n",
    "more= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(s1)-4)/len(s1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(a):\n",
    "    return a.replace(\" \", \"\")\n",
    "\n",
    "def seq_matcher(tgt_text,gt_text):\n",
    "    tgt_text = remove_space(tgt_text)\n",
    "    gt_text = remove_space(gt_text)\n",
    "    score = SequenceMatcher(None, gt_text, tgt_text).ratio()\n",
    "    mismatch_count = levenshtein(tgt_text, gt_text)\n",
    "    match_count = abs(max(len(gt_text),len(tgt_text))-mismatch_count)\n",
    "    score = match_count/max(len(gt_text),len(tgt_text))\n",
    "    return score\n",
    "def max_score(gt_text_lis,tgt_text):\n",
    "    score=0\n",
    "    gt_text_updated = \"\"\n",
    "    for gt_text in gt_text_lis:\n",
    "        tmp_score= seq_matcher(tgt_text,gt_text)\n",
    "        if score<tmp_score:\n",
    "            score = tmp_score\n",
    "            gt_text_updated = gt_text\n",
    "    return score,gt_text_updated\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_scores = []\n",
    "import ast\n",
    "file_confs = []\n",
    "below_thresh_score =0\n",
    "high_thresh_score=0\n",
    "high_conf_low_score=0\n",
    "high_score_low_conf=0\n",
    "thresh=90\n",
    "total_word=0\n",
    "for idx,page in enumerate(d['page_data']):\n",
    "    df_string = StringIO(d['page_data'][idx])\n",
    "    df = pd.read_csv(df_string, sep=\",\")\n",
    "    for idx,row in enumerate(df['conf_dict']):\n",
    "        \n",
    "        try:\n",
    "            score = df.iloc[idx]['score']\n",
    "            gt_text = df.iloc[idx]['text']\n",
    "            gt_text_lis = gt_text.split(' ')\n",
    "            confs = 0\n",
    "            row2 = ast.literal_eval(row)\n",
    "            for word_idx,word in enumerate(row2['text']):\n",
    "                text = row2['text'][word_idx]\n",
    "                conf = float(row2['conf'][word_idx])\n",
    "                score,gt_text = max_score(gt_text_lis,text)\n",
    "                file_confs.append(conf)\n",
    "#                 print(conf)\n",
    "#                 print(score*100)\n",
    "#                 print(\"ffffffffffffffff\")\n",
    "                file_scores.append(score*100)\n",
    "               # confs = confs + float(row2['conf'][word_idx])\n",
    "#             if len(row2['text'])!=0:\n",
    "#                 avg_conf = confs/len(row2['text'])\n",
    "#                 file_confs.append(avg_conf)\n",
    "#                 file_scores.append(score*100)\n",
    "                if score*100<thresh and conf<thresh:\n",
    "                    below_thresh_score+=1\n",
    "                    total_word+=1\n",
    "                elif score*100<thresh and conf>thresh:\n",
    "                    high_conf_low_score+=1\n",
    "                    total_word+=1\n",
    "                elif score*100>thresh and conf<thresh:\n",
    "                    high_score_low_conf+=1\n",
    "                    total_word+=1\n",
    "                elif score*100>thresh and conf>thresh:\n",
    "                    high_thresh_score+=1\n",
    "                    total_word+=1\n",
    "        except:\n",
    "            pass\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"below_thresh_score\", below_thresh_score/total_word)\n",
    "print(\"high_thresh_score\" , high_thresh_score/total_word)\n",
    "print(\"high_score_low_conf\", high_score_low_conf/total_word)\n",
    "print(\"high_conf_low_score\" , high_conf_low_score/total_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(file_scores,file_confs,'g') \n",
    "\n",
    "#plt.hist((file_scores,file_confs),label = (\"score\", \"confidence\"),bins=[0.5,1.0])\n",
    "plt.bar(file_scores, file_confs)\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SequenceMatcher(lambda x : x==\".\", s1, s2).ratio())\n",
    "seq = SequenceMatcher(lambda x : x==\".\", s1, s2)\n",
    "print(seq.find_longest_match(0,len(s1),0,len(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SequenceMatcher(None, s2, s1).ratio())\n",
    "seq = SequenceMatcher(lambda x : x==\".\", s2, s1)\n",
    "print(seq.find_longest_match(0,len(s2),0,len(s1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(188+110+17+30)/351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3='.modelandandsystem.finalizingevaluation.stackb.Iwas,'\n",
    "s2=\"modelandandsystemfinalizingevaluationstack..Iwas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = list(SequenceMatcher(None, s3, s2).get_matching_blocks())\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein(s2, s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in model.get_matches()['Similarity']:\n",
    "    if i!=0.0:\n",
    "        count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.get_matches()['Similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "43/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"modelandandsystem.finalizingevaluation.stackb.Iwas,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = \"modelandandsystemfinalizingevaluationstack..Iwas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = list(SequenceMatcher(None, s3, s).get_matching_blocks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good line : {'total_chars': 78024\t'total_match_chars': 77301\t'score': 0.9907336204244848}\n",
    "average line:  \t{'total_chars': 200192\t'total_match_chars': 182177\t'score': 0.9100113890664961}\n",
    "bad line : {'total_chars': 159366\t'total_match_chars': 141919\t'score': 0.8905224451890617}\n",
    "    \n",
    "    \n",
    "good_word:  {'total_chars': 78024\t'total_match_chars': 73859\t'score': 0.9466189890290168}\n",
    "average_word:  {'total_chars': 200192\t'total_match_chars': 187318\t'score': 0.9356917359335039}\n",
    "bad_word:  {'total_chars': 159358\t'total_match_chars': 143968\t'score': 0.9034249927835439}\n",
    "\n",
    "    \n",
    "good_craft_word:  {'total_chars': 78024\t'total_match_chars': 76006\t'score': 0.9741361632318261}\n",
    "average_craft_word: {'total_chars': 200148\t'total_match_chars': 190506\t'score': 0.9518256490197254}\n",
    "bad_craft_word:  {'total_chars': 159472\t'total_match_chars': 143604\t'score': 0.9004966389083977}\n",
    "\n",
    "good_craft_line:  {'total_chars': 78007\t'total_match_chars': 77143\t'score': 0.9889240709167126}\n",
    "average_craft_line: {'total_chars': 200164, 'total_match_chars': 173005, 'score': 0.8643162606662537}\n",
    "bad_craft_line:  {'total_chars': 159223\t'total_match_chars': 145184\t'score': 0.9118280650408547}\n",
    "    \n",
    "updated line with word ocr for table:\n",
    "good_line_craft: {'total_chars': 78024, 'total_match_chars': 77203, 'score': 0.9894775966369322}\n",
    "average_line_craft: {'total_chars': 200164\t'total_match_chars': 192864\t'score': 0.9635299054775085}\n",
    "bad_line_craft:  {'total_chars': 159473\t'total_match_chars': 151917\t'score': 0.9526189386291096}\n",
    "    \n",
    "updated line with line and dataframe:\n",
    "good_line_craft: {'total_chars': 78024, 'total_match_chars': 77100, 'score': 0.988157490003076}\n",
    "average_line_craft:{'total_chars': 200164\t'total_match_chars': 193285\t'score': 0.9656331807917508}\n",
    "bad_line_craft:  {'total_chars': 159473\t'total_match_chars': 151917\t'score': 0.9332363472186515}\n",
    "    \n",
    "updated line with line and string:\n",
    "good_line_craft: {'total_chars': 78024, 'total_match_chars': 77274, 'score': 0.9903875730544448}\n",
    "average_line_craft:{'total_chars': 200164\t'total_match_chars': 194295\t'score': 0.9706790431845886}\n",
    "bad_line_craft:  {'total_chars': 159473\t'total_match_chars': 151512\t'score': 0.9500793237726762}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "{'total_words': 10331\t'total_chars': 51109\t'total_match_chars': 48696\t'score': 0.9527871803400575\t'g_total_match_chars': 49480\t'g_score': 0.9681269443737893}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### english ocr evaluation\n",
    "good_craft_line_tesss:  {'total_words': 6846, 'total_chars': 34109, 'total_match_chars': 33829, 'score': 0.9917910228971826}\n",
    "good_craft_line_google:  {'total_words': 6846, 'total_chars': 34109, , 'g_total_match_chars': 33854, 'g_score': 0.9925239672813627}\n",
    "average_craft_line_tesss:  {'total_words': 9316'total_chars': 47852,'total_match_chars': 47037'score': 0.9829683189835325}\n",
    "average_craft_line_google:  {'total_words': 9316'total_chars': 47852, 'g_total_match_chars': 47190,'g_score': 0.9861656775056424}\n",
    "bad_craft_line_tesss:  {'total_words': 10336\t'total_chars': 51089\t'total_match_chars': 47752\t'score': 0.9346826126954921\t}\n",
    "bad_craft_line_google:  {'total_words': 10336\t'total_chars': 51089\t, 'g_total_match_chars': 48551\t'g_score': 0.9503219871205152}\n",
    "\n",
    "good_google_line_tesss:  {'total_words': 6846\t'total_chars': 34109\t'total_match_chars': 33901\t'score': 0.9939019027236213\t}\n",
    "good_google_line_google:  {'total_words': 6846\t'total_chars': 34109\t, 'g_total_match_chars': 33951\t'g_score': 0.9953677914919816}\n",
    "average_google_line_tesss:  {'total_words': 9319\t'total_chars': 47864\t'total_match_chars': 47147\t'score': 0.9850200568276785\t}\n",
    "average_google_line_google:  {'total_words': 9319\t'total_chars': 47864, 'g_total_match_chars': 47372\t'g_score': 0.9897208758148086}\n",
    "bad_google_line_tesss:  {'total_words': 10331\t'total_chars': 51109\t'total_match_chars': 48696\t'score': 0.9527871803400575\t}\n",
    "bad_google_line_google:  {'total_words': 10331\t'total_chars': 51109\t,'g_total_match_chars': 49480\t'g_score': 0.9681269443737893}\n",
    "   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## hindi ocr evaluation\n",
    "good_line_v1:   {'total_chars': 25527, 'total_match_chars': 23460, 'score': 0.9132185529047675}\n",
    "\n",
    "accuracy with dynamic adjustment\n",
    "good_line_v2:   {'total_chars': 25527, 'total_match_chars': 23731, 'score': 0.929643122967838}\n",
    "\n",
    "after horizontal merging integration in craft:\n",
    "good_craft_line_tess_model:    {'total_chars': 25527, 'total_match_chars': 24855, 'score': 0.9736749324244918}\n",
    "\n",
    "good_craft_line_indic_model:{'total_words': 6021, 'total_chars': 25527, 'total_match_chars': 21621, 'score': 0.8469855447173581}\n",
    "good_craft_word_indic_model: {'total_words': 6019, 'total_chars': 25527, 'total_match_chars': 20843, 'score': 0.816508011125475}\n",
    "    \n",
    "    \n",
    "good_craft_word_google_model:{'total_words': 6019, 'total_chars': 25527,  'g_total_match_chars': 24494, 'g_score': 0.9595330434441963}\n",
    "good_craft_word_tess_model:   {'total_words': 6019, 'total_chars': 25527, 'total_match_chars': 23699, 'score': 0.9283895483213852}\n",
    "good_craft_line_google_model:   {'total_words': 6021, 'total_chars': 25527, 'g_total_match_chars': 25237, 'g_score': 0.9886394797665217}\n",
    "\n",
    "    \n",
    "average_craft_line_tess_model:   {'total_words': 7203,'total_chars': 34204,'total_match_chars': 31215,'score': 0.9126125599345106}\n",
    "average_craft_line_google_model:   {'total_words': 7203,'total_chars': 34204,'g_total_match_chars': 32927\t'g_score': 0.9626651853584376}\n",
    "    \n",
    "bad_craft_line_tess_model:   {'total_words': 7012\t'total_chars': 33491\t'total_match_chars': 26700\t'score': 0.7972291063270729\t}\n",
    "bad_craft_line_google_model:  {'total_words': 7012\t'total_chars': 33491\t'g_total_match_chars': 30540\t'g_score': 0.9118867755516408}\n",
    "      \n",
    "average_craft_line_indic_model:   {'total_words': 7203\t'total_chars': 34223\t'total_match_chars': 25220\t'score': 0.7369313035093358\t}\n",
    "bad_craft_line_indic_model:   {'total_words': 7012\t'total_chars': 33491\t'total_match_chars': 21541\t'score': 0.6431877220745872\t}\n",
    "\n",
    "bad_craft_word_tess_model: {'total_words': 7005, 'total_chars': 33491, 'total_match_chars': 27652, 'score': 0.825654653488997} \n",
    "bad_craft_word_google_model:  {'total_words': 7005, 'total_chars': 33491,'g_total_match_chars': 30371, 'g_score': 0.9068406437550387} \n",
    "\n",
    "average_craft_word_tess_model: {'total_words': 7190\t'total_chars': 34221\t'total_match_chars': 29902\t'score': 0.8737909470792788\t} \n",
    "average_craft_word_google_model:  {'total_words': 7190\t'total_chars': 34221\t,'g_total_match_chars': 31197\t'g_score': 0.9116332076794951}\n",
    "    \n",
    "average_google_line_tess_model:   {'total_words': 7202\t'total_chars': 34196\t'total_match_chars': 31528\t'score': 0.9219791788513276\t}\n",
    "average_google_line_google_model:   {'total_words': 7202\t'total_chars': 34196\t,'g_total_match_chars': 33178\t'g_score': 0.9702304363083402}\n",
    "        \n",
    "bad_google_line_tess_model:   {'total_words': 6996\t'total_chars': 33424\t'total_match_chars': 27385\t'score': 0.8193214456677836\t}\n",
    "bad_google_line_google_model:  {'total_words': 6996\t'total_chars': 33424\t'g_total_match_chars': 30950\t'g_score': 0.9259813307802777}\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### ocr evaluation tamil\n",
    "\n",
    "\n",
    "good_craft_line_indic:  {'total_words': 3848, 'total_chars': 32767, 'total_match_chars': 31020, 'score': 0.9466841639454329}\n",
    "good_craft_line_google:   {'total_words': 3848, 'total_chars': 32767, 'g_total_match_chars': 31964, 'g_score': 0.9754936368907743}\n",
    "average_craft_line_indic: {'total_words': 3951, 'total_chars': 22381, 'total_match_chars': 19955, 'score': 0.8916044859479022}\n",
    "average_craft_line_google: {'total_words': 3951, 'total_chars': 22381,'g_total_match_chars': 21687, 'g_score': 0.9689915553371163}\n",
    "bad_craft_line_tesse: {'total_words': 5653, 'total_chars': 39796, 'total_match_chars': 36193, 'score': 0.9094632626394612}\n",
    "bad_craft_line_google: {'total_words': 5653, 'total_chars': 39796,'g_total_match_chars': 21687,'g_total_match_chars': 38257, 'g_score': 0.9613277716353402}\n",
    "     \n",
    "good_craft_line_tess: {'total_words': 3848, 'total_chars': 32767, 'total_match_chars': 31197, 'score': 0.9520859401226844}\n",
    "average_craft_line_tess: {'total_words': 3951, 'total_chars': 22381, 'total_match_chars': 20335, 'score': 0.908583173227291}\n",
    "bad_craft_line_indic: {'total_words': 5653, 'total_chars': 39796, 'total_match_chars': 36055, 'score': 0.9059955774449694}\n",
    "\n",
    "bad_craft_word_indic {'total_words': 3848, 'total_chars': 32767, 'total_match_chars': 25359, 'score': 0.7739188818018128}\n",
    "bad_craft_word_google: {'total_words': 3848, 'total_chars': 3276,,'g_total_match_chars': 26457, 'g_score': 0.8074282052064577}\n",
    "bad_craft_word_tess: {'total_words': 3848, 'total_chars': 32767, 'total_match_chars': 25589, 'score': 0.7809381389812922}\n",
    "\n",
    "    \n",
    "average_craft_word_indic: {'total_words': 3950, 'total_chars': 22381, 'total_match_chars': 17921, 'score': 0.8007238282471739}\n",
    "average_craft_word_google: {'total_words': 3950, 'total_chars': 22381,'g_total_match_chars': 19895, 'g_score': 0.8889236405879988}  \n",
    "average_craft_word_tess: {'total_words': 3950, 'total_chars': 22381, 'total_match_chars': 18441, 'score': 0.8239578213663376}\n",
    "\n",
    "good_craft_word_indic: {'total_words': 5647, 'total_chars': 39793, 'total_match_chars': 34343, 'score': 0.8630412384087653}\n",
    "good_craft_word_google: {'total_words': 5647, 'total_chars': 39793,'g_total_match_chars': 36204, 'g_score': 0.9098082577337723}  \n",
    "good_craft_word_tess: {'total_words': 5647, 'total_chars': 39793, 'total_match_chars': 34534, 'score': 0.8678410775764581}\n",
    "\n",
    "good_google_line_tess: {'total_words': 3848, 'total_chars': 32767, 'total_match_chars': 31375, 'score': 0.9575182348094119}\n",
    "average_google_line_tess: {'total_words': 3951, 'total_chars': 22383, 'total_match_chars': 20178, 'score': 0.9014877362283876}\n",
    "bad_google_line_tess: {'total_words': 5646, 'total_chars': 39599, 'total_match_chars': 36327, 'score': 0.9173716507992626}\n",
    "\n",
    "good_google_line_indic: {'total_words': 3848, 'total_chars': 32767, 'total_match_chars': 31471, 'score': 0.9604480117191077}\n",
    "average_google_line_indic: {'total_words': 3951, 'total_chars': 22383, 'total_match_chars': 20507, 'score': 0.9161863914578028}\n",
    "bad_google_line_indic: {'total_words': 5646, 'total_chars': 39599, 'total_match_chars': 36192, 'score': 0.9139624737998434}\n",
    "\n",
    "good_google_line_google: {'total_words': 3848, 'total_chars': 32767,  'g_total_match_chars': 32267, 'g_score': 0.9847407452620014}\n",
    "average_google_line_google: {'total_words': 3951, 'total_chars': 22383, 'g_total_match_chars': 21804, 'g_score': 0.9741321538667739}\n",
    "bad_google_line_google: {'total_words': 5646, 'total_chars': 39599, 'g_total_match_chars': 38179, 'g_score': 0.964140508598702}\n",
    "\n",
    "good_google_word_tess: {'total_words': 3848, 'total_chars': 32767, 'total_match_chars': 30459, 'score': 0.9295632801293985}\n",
    "average_google_word_tess: {'total_words': 3950, 'total_chars': 22381, 'total_match_chars': 19742, 'score': 0.8820874849202448}\n",
    "bad_google_word_tess: {'total_words': 5627, 'total_chars': 39609, 'total_match_chars': 36142, 'score': 0.9124693882703426}\n",
    "\n",
    "\n",
    "good_google_word_indic: {'total_words': 3848, 'total_chars': 32767, 'total_match_chars': 30695, 'score': 0.9367656483657338}\n",
    "average_google_word_indic: {'total_words': 3950, 'total_chars': 22381, 'total_match_chars': 20317, 'score': 0.90777891961932}\n",
    "bad_google_word_indic: {'total_words': 5627, 'total_chars': 39609, 'total_match_chars': 36120, 'score': 0.9119139589487237}\n",
    "\n",
    "good_google_word_google: {'total_words': 3848, 'total_chars': 32767, 'g_total_match_chars': 31601, 'g_score': 0.9644154179509873}\n",
    "average_google_word_google: {'total_words': 3950, 'total_chars': 22381, 'g_total_match_chars': 21673, 'g_score': 0.9683660247531388}\n",
    "bad_google_word_google: {'total_words': 5627, 'total_chars': 39609, 'g_total_match_chars': 37843, 'g_score': 0.9554141735464162}\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### ocr evaluation kannada\n",
    "\n",
    "\n",
    "good_craft_line_tesse:  {\"total_words\": 3401,'total_chars': 24497\t, 'total_match_chars': 23376\t 'score': 0.954239294607503\t}\n",
    "good_craft_line_google:   {\"total_words\": 3401,'total_chars': 24497\t,'g_total_match_chars': 23991\t 'g_score': 0.979344409519533}\n",
    "\n",
    "average_craft_line_tesse:  {'total_words': 4802\t 'total_chars': 33476\t 'total_match_chars': 31385\t 'score': 0.9375373401840125}\n",
    "average_craft_line_google:   {'total_words': 4802\t 'total_chars': 33476\t 'g_total_match_chars': 32518\t 'g_score': 0.9713824829728761}\n",
    "\n",
    "bad_craft_line_tesse:  {'total_words': 4455\t 'total_chars': 29789\t 'total_match_chars': 26543\t 'score': 0.8910336030078216}\n",
    "bad_craft_line_google:   {'total_words': 4455\t 'total_chars': 29789\t 'g_total_match_chars': 28634\t 'g_score': 0.9612272986672933}\n",
    "      \n",
    "good_craft_word_tesse:  {'total_words': 3401\t 'total_chars': 24497\t 'total_match_chars': 20137\t 'score': 0.8220190227374781}\n",
    "good_craft_word_google:   {'total_words': 3401\t 'total_chars': 24497\t  'g_total_match_chars': 19932\t 'g_score': 0.8136506511001347}\n",
    "\n",
    "average_craft_word_tesse:  {'total_words': 4802\t 'total_chars': 33476\t 'total_match_chars': 29141\t 'score': 0.8705042418449038}\n",
    "average_craft_word_google:   {'total_words': 4802\t 'total_chars': 33476\t 'g_total_match_chars': 28329\t 'g_score': 0.8462480583104314}\n",
    "\n",
    "bad_craft_word_tesse:  {'total_words': 4452\t 'total_chars': 29789\t 'total_match_chars': 24072\t 'score': 0.8080835207626976}\n",
    "bad_craft_word_google:   {'total_words': 4452\t 'total_chars': 29789\t'g_total_match_chars': 25572\t 'g_score': 0.8584376783376414}\n",
    "\n",
    "good_google_line_tesse:  {'total_words': 3401\t 'total_chars': 24497\t 'total_match_chars': 23551\t 'score': 0.96138302649304}\n",
    "good_google_line_google:   {'total_words': 3401\t 'total_chars': 24497\t 'g_total_match_chars': 24234\t 'g_score': 0.9892639915091644}\n",
    "\n",
    "average_google_line_tesse: {'total_words': 4801\t 'total_chars': 33470\t 'total_match_chars': 31849\t 'score': 0.9515685688676426}\n",
    "average_google_line_google:   {'total_words': 4801\t 'total_chars': 33470\t 'g_total_match_chars': 33037\t 'g_score': 0.9870630415297281}\n",
    "\n",
    "bad_google_line_tesse: {'total_words': 4455\t 'total_chars': 29789\t 'total_match_chars': 26842\t 'score': 0.9010708650844271}\n",
    "bad_google_line_google:   {'total_words': 4455\t 'total_chars': 29789\t 'g_total_match_chars': 28840\t 'g_score': 0.9681426029742523}\n",
    "\n",
    "good_google_word_tesse:  {'total_words': 3400\t 'total_chars': 24495\t 'total_match_chars': 21836\t 'score': 0.8914472341294142}\n",
    "good_google_word_google:   {'total_words': 3400\t 'total_chars': 24495\t 'g_total_match_chars': 21986\t 'g_score': 0.8975709328434375}\n",
    "\n",
    "average_google_word_tesse: {'total_words': 4801\t 'total_chars': 33470\t 'total_match_chars': 30428\t 'score': 0.9091126381834479}\n",
    "average_google_word_google:  {'total_words': 4801\t 'total_chars': 33470\t 'g_total_match_chars': 29172\t 'g_score': 0.8715864953689871}\n",
    "\n",
    "bad_google_word_tesse: {'total_words': 4452\t 'total_chars': 29789\t 'total_match_chars': 25816\t 'score': 0.8666286213031656}\n",
    "bad_google_word_google:   {'total_words': 4452\t 'total_chars': 29789\t 'g_total_match_chars': 26619\t 'g_score': 0.8935848803249522}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## tamil average shree\n",
    "processing started for page no.  0\n",
    "\n",
    "page level score {'total_chars': 826, 'total_match_chars': 740, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241811887.59696\n",
    "processing started for page no.  1\n",
    "page level score {'total_chars': 1163, 'total_match_chars': 1131, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241812449.641782\n",
    "processing started for page no.  2\n",
    "page level score {'total_chars': 1938, 'total_match_chars': 1924, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241812587.320466\n",
    "processing started for page no.  3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "page level score {'total_chars': 605, 'total_match_chars': 547, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241813056.306098\n",
    "processing started for page no.  4\n",
    "page level score {'total_chars': 1803, 'total_match_chars': 1785, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241813530.625787\n",
    "processing started for page no.  5\n",
    "page level score {'total_chars': 1390, 'total_match_chars': 1352, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241813736.493059\n",
    "\n",
    "processing completed for page in 3241824472.8735504\n",
    "processing started for page no.  1\n",
    "page level score {'total_chars': 784, 'total_match_chars': 734, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241825261.708538\n",
    "processing started for page no.  2\n",
    "page level score {'total_chars': 852, 'total_match_chars': 802, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241826577.777154\n",
    "processing started for page no.  3\n",
    "page level score {'total_chars': 1050, 'total_match_chars': 1038, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241827256.8978252\n",
    "processing started for page no.  4\n",
    "page level score {'total_chars': 718, 'total_match_chars': 637, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241827694.849202\n",
    "processing started for page no.  5\n",
    "page level score {'total_chars': 759, 'total_match_chars': 698, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241828478.2017365\n",
    "processing started for page no.  6\n",
    "page level score {'total_chars': 485, 'total_match_chars': 437, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241828927.7678914\n",
    "processing started for page no.  7\n",
    "page level score {'total_chars': 622, 'total_match_chars': 537, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241829308.369837\n",
    "processing started for page no.  8\n",
    "page level score {'total_chars': 691, 'total_match_chars': 625, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241830018.1830215\n",
    "\n",
    "\n",
    "\n",
    "page level score {'total_chars': 1711, 'total_match_chars': 1642, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241849648.098544\n",
    "processing started for page no.  1\n",
    "page level score {'total_chars': 1545, 'total_match_chars': 1502, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241849787.963669\n",
    "processing started for page no.  2\n",
    "page level score {'total_chars': 1652, 'total_match_chars': 1585, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241849934.5317726\n",
    "processing started for page no.  3\n",
    " page level score {'total_chars': 1677, 'total_match_chars': 1612, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241850090.2137995\n",
    "processing started for page no.  4\n",
    "page level score {'total_chars': 1927, 'total_match_chars': 1834, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241850241.3729124\n",
    "processing started for page no.  5\n",
    "page level score {'total_chars': 349, 'total_match_chars': 330, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241850335.562466\n",
    "processing started for page no.  6\n",
    "page level score {'total_chars': 1446, 'total_match_chars': 1358, 'g_total_match_chars': 0}\n",
    "processing completed for page in 3241850431.1224203\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = \"/home/naresh/Tarento/testing_document_processor/ocr_benchamark_data/tamil/curated_training_data/*/*.txt\"\n",
    "csv_path = \"/home/naresh/Tarento/testing_document_processor/ocr_benchamark_data/tamil/curated_training_data/*/*.csv\"\n",
    "\n",
    "for text_file, csv_file in zip(sorted(glob.glob(txt_path)),sorted(glob.glob(csv_path))):\n",
    "    \n",
    "    txt_file = open(text_file,\"r+\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df.reset_index(drop=True)\n",
    "    lines = txt_file.readlines()\n",
    "    for line in lines:\n",
    "        for idx, row in df.iterrows():\n",
    "            if row['key']==line. rstrip(\"\\n\"):\n",
    "                df = df.drop(index=idx,axis=0)\n",
    "    df.to_csv(csv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env2",
   "language": "python",
   "name": "ds-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
